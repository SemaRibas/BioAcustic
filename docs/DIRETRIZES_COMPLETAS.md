# Diretrizes para o Desenvolvimento de um Aplicativo de Reconhecimento BioacÃºstico de AnfÃ­bios com Deep Learning

**Autor:** Dr. Gemini (Especialista em IA e Desenvolvimento Fullstack)  
**Data:** 3 de novembro de 2025

---

## 1. Resumo (Abstract)

Este documento descreve o pipeline metodolÃ³gico necessÃ¡rio para a criaÃ§Ã£o de uma aplicaÃ§Ã£o web de deep learning (DL) destinada ao reconhecimento e classificaÃ§Ã£o de espÃ©cies de anfÃ­bios com base em suas vocalizaÃ§Ãµes (bioacÃºstica). 

O processo Ã© dividido em **seis fases principais**:

1. **AquisiÃ§Ã£o e Curadoria de Dados**
2. **PrÃ©-processamento de Ãudio e ExtraÃ§Ã£o de Features**
3. **Modelagem e Treinamento da Rede Neural**
4. **ConversÃ£o e OtimizaÃ§Ã£o do Modelo**
5. **Desenvolvimento da AplicaÃ§Ã£o de InferÃªncia**
6. **AvaliaÃ§Ã£o e IteraÃ§Ã£o**

---

## 2. Fase 1: AquisiÃ§Ã£o e Curadoria de Dados

Esta Ã© a **fase fundamental**. Modelos de deep learning sÃ£o tÃ£o bons quanto os dados que os alimentam.

### 2.1 Fonte de Dados

Necessitamos de um dataset de Ã¡udio robusto (`.wav`, `.mp3`, `.flac`).

#### RepositÃ³rios PÃºblicos
- **Xeno-canto** (https://xeno-canto.org/) - Maior biblioteca de sons de aves e anfÃ­bios
- **Macaulay Library** (Cornell Lab of Ornithology) - https://www.macaulaylibrary.org/
- **RepositÃ³rios cientÃ­ficos especÃ­ficos** de herpetologia
- **GBIF** (Global Biodiversity Information Facility) - dados associados

#### Coleta em Campo
- GravaÃ§Ãµes prÃ³prias sÃ£o valiosas
- Exigem esforÃ§o significativo de coleta
- Permitem controle sobre qualidade e condiÃ§Ãµes
- Requerem equipamento adequado (gravadores de alta fidelidade)

### 2.2 Qualidade e Balanceamento

#### Limpeza de Dados
Os dados devem ser de **alta fidelidade**, com o mÃ­nimo de ruÃ­do de fundo:
- âŒ Vento
- âŒ TrÃ¡fego
- âŒ VocalizaÃ§Ãµes de outras espÃ©cies sobrepostas
- âŒ Chuva intensa
- âœ… Isolamento claro da vocalizaÃ§Ã£o alvo

#### Balanceamento de Classes
Idealmente, devemos ter um **nÃºmero similar de amostras** para cada espÃ©cie-alvo.

**Problema de Desbalanceamento:**
```
Exemplo:
- Scinax fuscomarginatus: 5.000 amostras âœ…
- Boana cipoensis: 50 amostras âŒ
```

O modelo ficarÃ¡ **enviesado** para a classe majoritÃ¡ria.

**SoluÃ§Ãµes:**
- TÃ©cnicas de oversampling (SMOTE)
- Undersampling da classe majoritÃ¡ria
- Class weights durante o treinamento
- Data augmentation focada nas classes minoritÃ¡rias

### 2.3 AnotaÃ§Ã£o (Labeling)

#### Processo de AnotaÃ§Ã£o
Cada arquivo de Ã¡udio deve ser **rigorosamente anotado** com:
- Nome cientÃ­fico da espÃ©cie
- Timestamp (se aplicÃ¡vel)
- Contexto ambiental (opcional mas valioso)
- Qualidade da gravaÃ§Ã£o (rating)

#### ValidaÃ§Ã£o
- AnotaÃ§Ãµes devem ser **validadas por um especialista** (biÃ³logo ou herpetÃ³logo)
- Uso de ferramentas como Raven Pro ou Audacity para anÃ¡lise visual
- DocumentaÃ§Ã£o de incertezas

#### Estrutura Recomendada
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ Boana_faber/
â”‚   â”‚   â”œâ”€â”€ audio_001.wav
â”‚   â”‚   â”œâ”€â”€ audio_002.wav
â”‚   â”‚   â””â”€â”€ metadata.csv
â”‚   â”œâ”€â”€ Scinax_fuscomarginatus/
â”‚   â””â”€â”€ Boana_cipoensis/
â””â”€â”€ processed/
    â””â”€â”€ spectrograms/
```

### 2.4 Aumento de Dados (Data Augmentation)

Para aumentar a **robustez do modelo**, aplicamos transformaÃ§Ãµes Ã s amostras existentes:

#### TÃ©cnicas de Augmentation
1. **AdiÃ§Ã£o de RuÃ­do**
   - RuÃ­do branco (white noise)
   - RuÃ­do rosa (pink noise)
   - RuÃ­do ambiental real (chuva leve, folhas, insetos)

2. **Deslocamento de Tempo** (Time Shifting)
   - Deslocar o Ã¡udio em milissegundos
   - Simula variaÃ§Ãµes no timing de captura

3. **Deslocamento de Tom** (Pitch Shifting)
   - Â±2 semitons (cuidado para nÃ£o distorcer caracterÃ­sticas da espÃ©cie)
   - Simula variaÃ§Ãµes naturais ou equipamento

4. **MudanÃ§a de Velocidade** (Time Stretching)
   - Acelerar/desacelerar sem mudar o pitch
   - 0.9x a 1.1x

5. **Mixup**
   - Misturar duas amostras da mesma espÃ©cie
   - Criar hÃ­bridos sintÃ©ticos

#### âš ï¸ Cuidado com Over-augmentation
- NÃ£o criar dados artificiais que nÃ£o representam a realidade
- Manter caracterÃ­sticas acÃºsticas essenciais da espÃ©cie

---

## 3. Fase 2: PrÃ©-processamento e ExtraÃ§Ã£o de Features

Modelos de DL raramente consomem Ã¡udio "cru". Devemos convertÃª-lo em uma **representaÃ§Ã£o visual/numÃ©rica** que a rede neural possa entender.

### 3.1 PadronizaÃ§Ã£o

#### Taxa de Amostragem (Sample Rate)
Todo Ã¡udio deve ser **reamostrado** para uma taxa consistente:
- **22.050 Hz** (padrÃ£o para muitos modelos de Ã¡udio)
- **44.100 Hz** (qualidade CD, mais detalhes)
- **16.000 Hz** (suficiente para vocalizaÃ§Ãµes de baixa frequÃªncia)

**Escolha:** Depende da faixa de frequÃªncia das vocalizaÃ§Ãµes dos anfÃ­bios alvo.

#### NormalizaÃ§Ã£o de Amplitude
- Normalizar para o range [-1, 1]
- Prevenir clipping

### 3.2 SegmentaÃ§Ã£o

Ãudios longos de campo devem ser **segmentados** em clipes curtos:

```python
# Exemplo conceitual
duration = 3  # segundos
overlap = 0.5  # 50% de sobreposiÃ§Ã£o
```

- **DuraÃ§Ã£o recomendada:** 3 a 5 segundos
- **Overlap:** 0% a 50% (para capturar vocalizaÃ§Ãµes nas bordas)
- Descartar segmentos com apenas silÃªncio

### 3.3 ExtraÃ§Ã£o de Features (A Chave ğŸ”‘)

O Ã¡udio (sinal 1D) Ã© convertido em uma **representaÃ§Ã£o 2D**, tratada como uma "imagem".

#### Mel-Espectrograma (Recomendado)

**O que Ã©?**
- RepresentaÃ§Ã£o visual da **intensidade** (amplitude) de diferentes **frequÃªncias** (escala Mel) ao longo do **tempo**
- Escala Mel: mimetiza a percepÃ§Ã£o logarÃ­tmica humana de frequÃªncias

**Por que usar?**
- âœ… Standard na indÃºstria para tarefas de classificaÃ§Ã£o de Ã¡udio
- âœ… Captura informaÃ§Ãµes de tempo e frequÃªncia
- âœ… Reduz dimensionalidade mantendo informaÃ§Ãµes crÃ­ticas
- âœ… CompatÃ­vel com CNNs (tratado como imagem)

**ParÃ¢metros Importantes:**
```python
n_mels = 128  # NÃºmero de bandas Mel (altura da imagem)
n_fft = 2048  # Tamanho da janela FFT
hop_length = 512  # Stride entre janelas
fmin = 50  # FrequÃªncia mÃ­nima (Hz)
fmax = 8000  # FrequÃªncia mÃ¡xima (Hz)
```

#### Biblioteca Python: `librosa`

```python
import librosa
import librosa.display
import numpy as np

# Carregar Ã¡udio
y, sr = librosa.load('audio.wav', sr=22050)

# Gerar Mel-Espectrograma
mel_spec = librosa.feature.melspectrogram(
    y=y, 
    sr=sr, 
    n_mels=128, 
    n_fft=2048, 
    hop_length=512
)

# Converter para escala dB (logarÃ­tmica)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
```

#### Alternativas (AvanÃ§adas)
- **MFCCs** (Mel-Frequency Cepstral Coefficients) - mais compacto
- **Spectrograma Linear** - sem escala Mel
- **CQT** (Constant-Q Transform) - melhor para tons musicais

### 3.4 Pipeline Completo de PrÃ©-processamento

```
Ãudio Raw â†’ Resample â†’ Segmentar â†’ Normalizar â†’ Mel-Espectrograma â†’ Salvar como .npy/.png
```

---

## 4. Fase 3: Modelagem e Treinamento (O Backend de IA)

Nesta fase, tratamos o problema como uma **"classificaÃ§Ã£o de imagens"** (os espectrogramas sÃ£o as imagens).

### 4.1 Arquitetura do Modelo

#### Redes Neurais Convolucionais (CNNs)

**Por que CNNs?**
- Excelentes para dados 2D (imagens/espectrogramas)
- Aprendem caracterÃ­sticas hierÃ¡rquicas automaticamente
- Compartilhamento de pesos reduz parÃ¢metros

#### Transfer Learning (Recomendado)

Em vez de construir do **zero**, usamos arquiteturas **prÃ©-treinadas no ImageNet** e as **re-treinamos** (fine-tuning) para nossos espectrogramas.

**Modelos Leves (Ideais para Web):**

1. **MobileNetV2**
   - âœ… Leve (14 MB)
   - âœ… RÃ¡pido
   - âœ… Otimizado para dispositivos mÃ³veis
   - ParÃ¢metros: ~3.5M

2. **EfficientNetB0**
   - âœ… Melhor acurÃ¡cia por tamanho
   - âœ… EscalonÃ¡vel (B0 a B7)
   - ParÃ¢metros: ~5.3M

3. **ResNet50** (se precisar de mais capacidade)
   - Mais pesado (98 MB)
   - Maior acurÃ¡cia potencial

#### Modelos EspecÃ­ficos de Ãudio (AvanÃ§ado)

- **PANNs** (Large-Scale Pre-trained Audio Neural Networks)
  - PrÃ©-treinados em AudioSet
  - Performance superior para tarefas de Ã¡udio
  - https://github.com/qiuqiangkong/audioset_tagging_cnn

- **VGGish** (Google)
  - PrÃ©-treinado para embeddings de Ã¡udio

### 4.2 Arquitetura Proposta

```python
# PseudocÃ³digo TensorFlow/Keras

base_model = tf.keras.applications.MobileNetV2(
    input_shape=(128, 128, 3),  # Altura, Largura, Canais
    include_top=False,
    weights='imagenet'
)

# Congelar as camadas base (opcional)
base_model.trainable = False

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
```

### 4.3 Ambiente de Treinamento

#### Hardware
- **GPU:** NVIDIA (CUDA compatÃ­vel)
  - GTX 1660 Ti (mÃ­nimo)
  - RTX 3060 (recomendado)
  - A100/V100 (para datasets grandes)
- **RAM:** 16 GB mÃ­nimo
- **Armazenamento:** SSD (para I/O rÃ¡pido)

#### Software
- **Python:** 3.8+
- **TensorFlow:** 2.10+
- **Keras:** Integrado ao TensorFlow
- **CUDA/cuDNN:** Para aceleraÃ§Ã£o GPU

#### Alternativas Cloud
- **Google Colab** (GPU gratuita, 12h de sessÃ£o)
- **Kaggle Notebooks** (30h/semana de GPU)
- **Google Cloud AI Platform**
- **AWS SageMaker**

### 4.4 Processo de Treinamento

#### Split de Dados
```
Dataset:
â”œâ”€â”€ Treino: 70%
â”œâ”€â”€ ValidaÃ§Ã£o: 15%
â””â”€â”€ Teste: 15%
```

#### HiperparÃ¢metros
```python
batch_size = 32
epochs = 50
learning_rate = 0.0001
optimizer = Adam
loss = categorical_crossentropy
```

#### Callbacks
```python
callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.5, patience=5),
    ModelCheckpoint('best_model.h5', save_best_only=True)
]
```

#### MÃ©tricas
- **AcurÃ¡cia** (Accuracy)
- **Precision, Recall, F1-Score** (por classe)
- **Matriz de ConfusÃ£o**
- **Top-3 Accuracy** (Ãºtil para espÃ©cies similares)

### 4.5 SaÃ­da (O "Artefato")

O resultado desta fase Ã© um **arquivo de modelo treinado**:

- **Formato Keras:** `anfibios_model.h5` (HDF5)
- **Formato SavedModel:** `saved_model/` (diretÃ³rio)
  - PreferÃ­vel para deployment
  - ContÃ©m arquitetura + pesos + assets

```
backend/models/
â””â”€â”€ anfibios_classifier/
    â”œâ”€â”€ saved_model.pb
    â”œâ”€â”€ variables/
    â”‚   â”œâ”€â”€ variables.data-00000-of-00001
    â”‚   â””â”€â”€ variables.index
    â””â”€â”€ assets/
        â””â”€â”€ class_labels.json
```

---

## 5. Fase 4: ConversÃ£o e OtimizaÃ§Ã£o do Modelo

O modelo Python (`.h5`/`SavedModel`) **nÃ£o pode ser usado diretamente no navegador** (JavaScript).

### 5.1 Ferramenta: TensorFlow.js Converter

#### InstalaÃ§Ã£o
```bash
pip install tensorflowjs
```

#### ConversÃ£o

```bash
tensorflowjs_converter \
    --input_format=keras \
    ./backend/models/anfibios_model.h5 \
    ./frontend/assets/model/
```

**Ou para SavedModel:**
```bash
tensorflowjs_converter \
    --input_format=tf_saved_model \
    ./backend/models/anfibios_classifier/ \
    ./frontend/assets/model/
```

#### OpÃ§Ãµes de OtimizaÃ§Ã£o

```bash
tensorflowjs_converter \
    --input_format=keras \
    --quantize_uint8 \  # QuantizaÃ§Ã£o para reduzir tamanho
    --weight_shard_size_bytes=4194304 \  # 4MB shards
    ./backend/models/anfibios_model.h5 \
    ./frontend/assets/model/
```

**TÃ©cnicas de OtimizaÃ§Ã£o:**
- **QuantizaÃ§Ã£o (uint8/uint16):** Reduz tamanho em ~4x com perda mÃ­nima de acurÃ¡cia
- **Pruning:** Remove conexÃµes nÃ£o importantes
- **Weight Clustering:** Agrupa pesos similares

### 5.2 SaÃ­da

```
frontend/assets/model/
â”œâ”€â”€ model.json                 # Arquitetura da rede
â”œâ”€â”€ group1-shard1of3.bin       # Pesos (shard 1)
â”œâ”€â”€ group1-shard2of3.bin       # Pesos (shard 2)
â””â”€â”€ group1-shard3of3.bin       # Pesos (shard 3)
```

- **`model.json`:** Descreve a arquitetura (camadas, shapes, conexÃµes)
- **`.bin` files:** Pesos do modelo divididos em arquivos menores (shards)

### 5.3 VerificaÃ§Ã£o

```javascript
// Testar carregamento no navegador
const model = await tf.loadLayersModel('./assets/model/model.json');
console.log('Modelo carregado:', model);
```

---

## 6. Fase 5: Desenvolvimento da AplicaÃ§Ã£o de InferÃªncia (O Frontend)

Esta Ã© a fase que desenvolve a **interface do usuÃ¡rio**. A aplicaÃ§Ã£o **nÃ£o treina** o modelo; ela **usa** o modelo da Fase 4 para fazer **previsÃµes** (inferÃªncia).

### 6.1 Stack de Tecnologia

#### Core
- **HTML5:** Estrutura semÃ¢ntica
- **Tailwind CSS:** EstilizaÃ§Ã£o utilitÃ¡ria, responsiva
- **JavaScript (ESM):** LÃ³gica moderna, modular
- **TensorFlow.js (TF.js):** Biblioteca principal do Google para rodar ML no navegador

#### Bibliotecas Auxiliares
- **meyda.js:** ExtraÃ§Ã£o de features de Ã¡udio no navegador
- **wavesurfer.js:** VisualizaÃ§Ã£o de forma de onda (opcional)
- **Tone.js:** ManipulaÃ§Ã£o avanÃ§ada de Ã¡udio (opcional)

### 6.2 LÃ³gica da AplicaÃ§Ã£o

#### 6.2.1 Carregamento do Modelo

```javascript
import * as tf from '@tensorflow/tfjs';

let model;

async function loadModel() {
    try {
        model = await tf.loadLayersModel('./assets/model/model.json');
        console.log('âœ… Modelo carregado com sucesso');
        console.log('Input shape:', model.inputs[0].shape);
    } catch (error) {
        console.error('âŒ Erro ao carregar modelo:', error);
    }
}

loadModel();
```

#### 6.2.2 Entrada do UsuÃ¡rio

**OpÃ§Ã£o 1: Upload de Arquivo**
```html
<input type="file" id="audioUpload" accept="audio/*">
```

```javascript
document.getElementById('audioUpload').addEventListener('change', async (e) => {
    const file = e.target.files[0];
    const audioBuffer = await file.arrayBuffer();
    processAudio(audioBuffer);
});
```

**OpÃ§Ã£o 2: GravaÃ§Ã£o ao Vivo**
```javascript
const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
const mediaRecorder = new MediaRecorder(stream);
// ... lÃ³gica de gravaÃ§Ã£o
```

#### 6.2.3 PrÃ©-processamento em JavaScript (ğŸ”¥ O Desafio)

Esta Ã© a **parte mais complexa** do frontend. Devemos **replicar exatamente** o mesmo prÃ©-processamento da Fase 2 (Python/librosa) usando JavaScript.

**Problema:** `librosa` nÃ£o existe em JavaScript nativamente.

**SoluÃ§Ãµes:**

##### OpÃ§Ã£o A: Backend API (Recomendado para ProduÃ§Ã£o)
```
Cliente â†’ Upload Ãudio â†’ API Python â†’ PrÃ©-processamento â†’ Retorna Tensor â†’ Frontend â†’ InferÃªncia
```

Vantagens:
- âœ… Usa `librosa` diretamente
- âœ… Processamento consistente
- âœ… Menos cÃ³digo no frontend

Desvantagens:
- âŒ Requer servidor
- âŒ LatÃªncia de rede

##### OpÃ§Ã£o B: PrÃ©-processamento no Navegador (100% Cliente)

Usando **Web Audio API** + **FFT Manual**:

```javascript
// PseudocÃ³digo complexo
async function audioToMelSpectrogram(audioBuffer) {
    const audioContext = new AudioContext();
    const audioBufferNode = audioContext.createBufferSource();
    
    // 1. Reamostrar para 22050 Hz
    const resampled = resampleAudio(audioBuffer, 22050);
    
    // 2. Aplicar STFT (Short-Time Fourier Transform)
    const stft = computeSTFT(resampled, {
        n_fft: 2048,
        hop_length: 512
    });
    
    // 3. Converter para escala Mel
    const melFilterbank = createMelFilterbank(128, 22050, 50, 8000);
    const melSpec = applyMelFilterbank(stft, melFilterbank);
    
    // 4. Converter para dB
    const melSpecDB = powerToDB(melSpec);
    
    // 5. Normalizar
    const normalized = normalize(melSpecDB);
    
    return normalized;
}
```

**Bibliotecas Auxiliares:**
- **meyda.js:** Pode extrair MFCCs e algumas features
- **ImplementaÃ§Ã£o customizada de Mel-Filterbank**

âš ï¸ **Importante:** O espectrograma gerado em JS **DEVE** ser idÃªntico ao gerado em Python, pixel por pixel, senÃ£o o modelo falharÃ¡.

#### 6.2.4 InferÃªncia

```javascript
async function predictSpecies(audioFile) {
    // 1. PrÃ©-processar Ã¡udio â†’ espectrograma
    const melSpec = await audioToMelSpectrogram(audioFile);
    
    // 2. Converter para tensor (shape: [1, 128, 128, 3])
    const inputTensor = tf.tensor4d([melSpec]);
    
    // 3. Fazer prediÃ§Ã£o
    const predictions = model.predict(inputTensor);
    
    // 4. Obter probabilidades
    const probabilities = await predictions.data();
    
    // 5. Limpar memÃ³ria
    inputTensor.dispose();
    predictions.dispose();
    
    return probabilities;
}
```

#### 6.2.5 PÃ³s-processamento

```javascript
const classNames = [
    'Boana faber',
    'Scinax fuscomarginatus',
    'Boana cipoensis',
    'Dendropsophus minutus'
    // ... outras espÃ©cies
];

function postprocess(probabilities) {
    // Mapear para nomes de espÃ©cies
    const results = classNames.map((name, i) => ({
        species: name,
        probability: probabilities[i],
        confidence: (probabilities[i] * 100).toFixed(2) + '%'
    }));
    
    // Ordenar por probabilidade
    results.sort((a, b) => b.probability - a.probability);
    
    return results.slice(0, 5);  // Top 5
}
```

#### 6.2.6 Interface de UsuÃ¡rio

```html
<div id="results" class="mt-6">
    <h3 class="text-xl font-bold mb-4">Resultados da ClassificaÃ§Ã£o:</h3>
    <div id="top-predictions" class="space-y-3">
        <!-- Dinamicamente populado -->
    </div>
</div>
```

```javascript
function displayResults(predictions) {
    const container = document.getElementById('top-predictions');
    container.innerHTML = predictions.map(pred => `
        <div class="bg-white p-4 rounded-lg shadow-md border-l-4 border-green-500">
            <div class="flex justify-between items-center">
                <span class="font-semibold text-lg">${pred.species}</span>
                <span class="text-2xl font-bold text-green-600">${pred.confidence}</span>
            </div>
            <div class="w-full bg-gray-200 rounded-full h-2.5 mt-2">
                <div class="bg-green-600 h-2.5 rounded-full" style="width: ${pred.confidence}"></div>
            </div>
        </div>
    `).join('');
}
```

### 6.3 Estrutura de Arquivos Frontend

```
frontend/
â”œâ”€â”€ index.html
â”œâ”€â”€ css/
â”‚   â””â”€â”€ styles.css (se nÃ£o usar Tailwind CDN)
â”œâ”€â”€ js/
â”‚   â”œâ”€â”€ app.js (entry point)
â”‚   â”œâ”€â”€ model.js (carregamento e inferÃªncia)
â”‚   â”œâ”€â”€ audio.js (processamento de Ã¡udio)
â”‚   â””â”€â”€ ui.js (manipulaÃ§Ã£o DOM)
â””â”€â”€ assets/
    â”œâ”€â”€ model/
    â”‚   â”œâ”€â”€ model.json
    â”‚   â””â”€â”€ *.bin
    â””â”€â”€ images/
```

### 6.4 OtimizaÃ§Ãµes de Performance

#### Lazy Loading do Modelo
```javascript
let modelPromise;

function getModel() {
    if (!modelPromise) {
        modelPromise = tf.loadLayersModel('./assets/model/model.json');
    }
    return modelPromise;
}
```

#### Web Workers
Para nÃ£o bloquear a UI durante processamento pesado:
```javascript
const worker = new Worker('audio-processor.worker.js');
worker.postMessage({ audio: audioBuffer });
worker.onmessage = (e) => {
    const melSpec = e.data;
    predict(melSpec);
};
```

#### Warmup do Modelo
```javascript
async function warmupModel() {
    const dummyInput = tf.zeros([1, 128, 128, 3]);
    await model.predict(dummyInput);
    dummyInput.dispose();
}
```

---

## 7. Fase 6: AvaliaÃ§Ã£o e IteraÃ§Ã£o

### 7.1 Teste de Campo

A aplicaÃ§Ã£o deve ser testada com **novos dados de Ã¡udio** (que **nÃ£o estavam** no dataset de treino) para verificar sua acurÃ¡cia no mundo real.

#### Protocolo de Teste

1. **Dataset de Teste Isolado**
   - Nunca visto pelo modelo durante treino
   - Representativo de condiÃ§Ãµes reais de campo

2. **MÃ©tricas de AvaliaÃ§Ã£o**
   ```python
   from sklearn.metrics import classification_report, confusion_matrix
   
   y_true = [...]  # Labels verdadeiros
   y_pred = [...]  # PrediÃ§Ãµes do modelo
   
   print(classification_report(y_true, y_pred, target_names=class_names))
   ```

3. **AnÃ¡lise de Erros**
   - Quais espÃ©cies sÃ£o confundidas?
   - Por que o modelo erra?
   - CaracterÃ­sticas acÃºsticas similares?

4. **Teste com UsuÃ¡rios Reais**
   - Beta testers (biÃ³logos, pesquisadores)
   - Coleta de feedback qualitativo
   - Usabilidade da interface

### 7.2 MÃ©tricas de Performance

#### AcurÃ¡cia por EspÃ©cie
```
Boana faber:            95% (excelente)
Scinax fuscomarginatus: 88% (bom)
Boana cipoensis:        62% (precisa melhorar)
```

#### Matriz de ConfusÃ£o
```
                Previsto
Real        Sp1   Sp2   Sp3
Sp1         150    10     5
Sp2          12   140     8
Sp3          20    15   100
```

#### Tempo de InferÃªncia
- **Meta:** < 2 segundos do upload atÃ© resultado
- Medir em diferentes dispositivos (desktop, mobile, tablet)

### 7.3 Feedback e IteraÃ§Ãµes

#### Ciclo de Melhoria ContÃ­nua

```
Feedback â†’ AnÃ¡lise â†’ HipÃ³tese â†’ ModificaÃ§Ã£o â†’ Re-treino â†’ Deploy â†’ Teste
```

#### Ãreas de Ajuste

**Na Fase 3 (Modelo):**
- Arquitetura diferente (trocar de MobileNet para EfficientNet)
- Mais Ã©pocas de treinamento
- Ajuste de hiperparÃ¢metros
- Mais data augmentation
- TÃ©cnicas de regularizaÃ§Ã£o (dropout, L2)

**Na Fase 5 (UI/UX):**
- Interface mais intuitiva
- Feedback visual melhorado
- InstruÃ§Ãµes mais claras
- AdiÃ§Ã£o de modo "DÃºvida" (quando confianÃ§a < 70%)

**Na Fase 1 (Dados):**
- Coletar mais dados das espÃ©cies com baixa acurÃ¡cia
- Limpar dados com ruÃ­do excessivo
- Re-balancear dataset

### 7.4 Monitoramento em ProduÃ§Ã£o

#### Logging
```javascript
// Enviar prediÃ§Ãµes para analytics
function logPrediction(species, confidence) {
    fetch('/api/log', {
        method: 'POST',
        body: JSON.stringify({
            predicted_species: species,
            confidence: confidence,
            timestamp: new Date().toISOString(),
            user_feedback: null  // Preenchido depois
        })
    });
}
```

#### Coleta de Feedback do UsuÃ¡rio
```html
<div>
    <p>Esta prediÃ§Ã£o foi Ãºtil?</p>
    <button onclick="feedbackCorrect()">âœ… Correto</button>
    <button onclick="feedbackIncorrect()">âŒ Incorreto</button>
</div>
```

Usar esse feedback para:
- Identificar casos problemÃ¡ticos
- Criar dataset de "hard examples"
- Re-treinar modelo com exemplos difÃ­ceis

### 7.5 Versionamento do Modelo

```
models/
â”œâ”€â”€ v1.0.0/  (baseline)
â”œâ”€â”€ v1.1.0/  (+ data augmentation)
â”œâ”€â”€ v1.2.0/  (arquitetura otimizada)
â””â”€â”€ v2.0.0/  (re-treinado com novos dados)
```

Manter histÃ³rico de:
- AcurÃ¡cia de cada versÃ£o
- Dataset usado
- HiperparÃ¢metros
- Data de deploy

---

## 8. Arquitetura Final do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USUÃRIO                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Navegador)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   HTML5    â”‚  â”‚  Tailwind    â”‚  â”‚   JavaScript (ESM)      â”‚ â”‚
â”‚  â”‚  Interface â”‚  â”‚     CSS      â”‚  â”‚   + TensorFlow.js       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Modelo TF.js (model.json + shards.bin)                  â”‚  â”‚
â”‚  â”‚  PrÃ©-processamento de Ãudio (Web Audio API)              â”‚  â”‚
â”‚  â”‚  InferÃªncia Local (Privacidade!)                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–²
                      â”‚ (Modelo convertido)
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (Python)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Fase 1-2: Curadoria de Dados + PrÃ©-processamento         â”‚ â”‚
â”‚  â”‚  (librosa, numpy, pandas)                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Fase 3: Treinamento do Modelo                             â”‚ â”‚
â”‚  â”‚  (TensorFlow/Keras + GPU)                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Fase 4: ConversÃ£o para TF.js                              â”‚ â”‚
â”‚  â”‚  (tensorflowjs_converter)                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Roadmap de ImplementaÃ§Ã£o

### Sprint 1: FundaÃ§Ã£o (2-3 semanas)
- [ ] Configurar ambiente Python (virtual env, dependÃªncias)
- [ ] Baixar dataset inicial (Xeno-canto)
- [ ] Scripts de curadoria bÃ¡sica
- [ ] Implementar pipeline de prÃ©-processamento (Fase 2)
- [ ] Validar espectrogramas visualmente

### Sprint 2: Modelagem (3-4 semanas)
- [ ] Implementar modelo baseline (MobileNetV2)
- [ ] Treinar primeira versÃ£o (v1.0.0)
- [ ] Avaliar mÃ©tricas no conjunto de teste
- [ ] Iterar com data augmentation
- [ ] Salvar melhor modelo

### Sprint 3: ConversÃ£o e Frontend (2-3 semanas)
- [ ] Converter modelo para TensorFlow.js
- [ ] Criar interface HTML base
- [ ] Implementar upload de Ã¡udio
- [ ] Integrar modelo TF.js
- [ ] Testar inferÃªncia no navegador

### Sprint 4: Refinamento (2-3 semanas)
- [ ] Implementar prÃ©-processamento em JS (ou API backend)
- [ ] Melhorar UI/UX com Tailwind
- [ ] Adicionar visualizaÃ§Ã£o de espectrograma
- [ ] Testes em diferentes navegadores
- [ ] OtimizaÃ§Ãµes de performance

### Sprint 5: ValidaÃ§Ã£o (2 semanas)
- [ ] Testes de campo com usuÃ¡rios reais
- [ ] AnÃ¡lise de erros detalhada
- [ ] Coleta de feedback
- [ ] DocumentaÃ§Ã£o final

### Sprint 6: Deploy (1 semana)
- [ ] Hospedar frontend (Netlify, Vercel, GitHub Pages)
- [ ] Configurar domÃ­nio
- [ ] Analytics e logging
- [ ] LanÃ§amento beta

---

## 10. Recursos e ReferÃªncias

### Datasets
- ğŸŒ **Xeno-canto:** https://xeno-canto.org/
- ğŸŒ **Macaulay Library:** https://www.macaulaylibrary.org/
- ğŸ“š **ESC-50** (para treino de baseline): https://github.com/karolpiczak/ESC-50

### Bibliotecas Python
- ğŸ”Š **librosa:** https://librosa.org/
- ğŸ§  **TensorFlow:** https://www.tensorflow.org/
- ğŸ“Š **scikit-learn:** https://scikit-learn.org/

### Bibliotecas JavaScript
- ğŸ§  **TensorFlow.js:** https://www.tensorflow.org/js
- ğŸ”Š **meyda.js:** https://meyda.js.org/
- ğŸµ **Tone.js:** https://tonejs.github.io/

### Papers CientÃ­ficos
1. **"Automatic acoustic identification of individuals in multiple species"** (2020)
2. **"Deep learning for bioacoustic classification"** (2019)
3. **"CNN Architectures for Large-Scale Audio Classification"** (Google Research, 2017)

### Tutoriais
- ğŸ“ TensorFlow Audio Recognition: https://www.tensorflow.org/tutorials/audio/simple_audio
- ğŸ“ Audio Classification with TF.js: https://blog.tensorflow.org/2019/06/audio-classification-tensorflow-js.html

---

## 11. ConsideraÃ§Ãµes Finais

### Desafios TÃ©cnicos Principais

1. **Qualidade dos Dados**
   - Maior gargalo do projeto
   - Exige curadoria manual intensiva

2. **PrÃ©-processamento no Navegador**
   - Replicar `librosa` em JavaScript Ã© complexo
   - Considerar API backend como alternativa

3. **Performance em Dispositivos MÃ³veis**
   - Modelos leves sÃ£o essenciais
   - Testar em hardware variado

4. **GeneralizaÃ§Ã£o**
   - Modelo pode ter overfitting no dataset de treino
   - Teste com dados do mundo real Ã© crÃ­tico

### Boas PrÃ¡ticas

- âœ… **Versionamento:** Git para cÃ³digo, DVC para dados
- âœ… **DocumentaÃ§Ã£o:** Docstrings, comentÃ¡rios, README
- âœ… **Testes:** Unit tests para funÃ§Ãµes crÃ­ticas
- âœ… **Reprodutibilidade:** Seeds aleatÃ³rias fixas, environment.yml
- âœ… **Ã‰tica:** Respeitar licenÃ§as de dados, crÃ©ditos aos autores

### PrÃ³ximos Passos

1. Iniciar com **proof of concept** (3-5 espÃ©cies comuns)
2. Validar pipeline completo end-to-end
3. Escalar para mais espÃ©cies gradualmente
4. Envolver comunidade cientÃ­fica para validaÃ§Ã£o

---

## GlossÃ¡rio

- **BioacÃºstica:** Estudo dos sons produzidos por organismos vivos
- **CNN:** Convolutional Neural Network (Rede Neural Convolucional)
- **Espectrograma:** RepresentaÃ§Ã£o visual de frequÃªncias ao longo do tempo
- **FFT:** Fast Fourier Transform (Transformada RÃ¡pida de Fourier)
- **Mel Scale:** Escala perceptual de frequÃªncias
- **InferÃªncia:** Processo de fazer prediÃ§Ãµes com um modelo treinado
- **Transfer Learning:** Reutilizar modelo prÃ©-treinado para nova tarefa
- **TensorFlow.js:** Biblioteca para rodar modelos de ML no navegador

---

**Documento vivo - VersÃ£o 1.0**  
Ãšltima atualizaÃ§Ã£o: 3 de novembro de 2025

Para contribuiÃ§Ãµes ou dÃºvidas, abra uma issue no repositÃ³rio do projeto.
